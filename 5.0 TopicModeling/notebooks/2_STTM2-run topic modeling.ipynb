{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tokenize import tokenize\n",
    "from clean_tokenizer import tokenize_tweets\n",
    "\n",
    "# Import module from gsdmm repository\n",
    "sys.path.insert(0, '../gsdmm/')\n",
    "from gsdmm import MovieGroupProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import preprocessed clean tweet data \n",
    "data_dir = r\"E:\\Project\\Data\\tweet_data\\5.0 Tokenized_Topic_Modeling\\clean_local_time_2019-03_2020-02_state_level_tokenized.csv\"\n",
    "tweets_df = pd.read_csv(data_dir) \n",
    "tweets_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Term Text Modeling (STTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['clean_tweet'] = tweets_df['clean_tweet'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cleaned tweet into tokens list\n",
    "tweets_df['clean_tokens'] = tweets_df.clean_tweet.apply(lambda x: re.split('\\s', x))\n",
    "\n",
    "# Create list of tweet tokens\n",
    "docs = tweets_df['clean_tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train STTM model\n",
    "# Init of the Gibbs Sampling Dirichlet Mixture Model algorithm\n",
    "# K = number of potential topic (which we don't know a priori)\n",
    "# alpha = \n",
    "# beta = \n",
    "# n_iters = number of iterations to \n",
    "mgp = MovieGroupProcess(K=6, alpha=0.1, beta=0.1, n_iters=30)\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)\n",
    "y = mgp.fit(docs, n_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "with open(r'C:\\Users\\Administrator\\Desktop\\topic-modeling-health-tweets-master\\dumps\\trained_models\\5clusters_2019.model', 'wb') as f:\n",
    "    pickle.dump(mgp, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in trained model with 10 topics \n",
    "filehandler = open(r'C:\\Users\\Administrator\\Desktop\\topic-modeling-health-tweets-master\\dumps\\trained_models\\5clusters_2019.model', 'rb')\n",
    "mgp = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    '''prints the top words in each cluster'''\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts =sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print('Cluster %s : %s'%(cluster,sort_dicts))\n",
    "        print(' — — — — — — — — —')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "print('*'*20)\n",
    "\n",
    "# Topics sorted by the number of document they are allocated to\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)\n",
    "\n",
    "# Show the top 5 words in term frequency for each cluster \n",
    "topic_indices = np.arange(start=0, stop=len(doc_count), step=1)\n",
    "top_words(mgp.cluster_word_distribution, topic_indices, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def cluster_importance(mgp):\n",
    "    '''returns a word-topic matrix[phi] where each value represents\n",
    "    the word importance for that particular cluster; \n",
    "    phi[i][w] would be the importance of word w in topic i.\n",
    "    '''\n",
    "    n_z_w = mgp.cluster_word_distribution\n",
    "    beta, V, K = mgp.beta, mgp.vocab_size, mgp.K\n",
    "    phi = [{} for i in range(K)]        \n",
    "    for z in range(K):\n",
    "        for w in n_z_w[z]:\n",
    "            phi[z][w] = (n_z_w[z][w]+beta)/(sum(n_z_w[z].values())+V*beta)\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function(s)\n",
    "def get_topic_name(doc, topic_dict):\n",
    "    '''returns the topic name string value from a dictionary of topics'''\n",
    "    topic_desc = topic_dict[doc]\n",
    "    return topic_desc\n",
    "\n",
    "def topic_allocation(df, docs, mgp, topic_dict):\n",
    "    '''allocates all topics to each document in original dataframe,\n",
    "    adding two columns for cluster number and cluster description'''\n",
    "    topic_allocations=[]\n",
    "    for doc in tqdm(docs):\n",
    "        topic_label, score = mgp.choose_best_label(doc)\n",
    "        topic_allocations.append(topic_label)\n",
    "\n",
    "    df['dominant_topic'] = topic_allocations\n",
    "    \n",
    "    df['topic_name'] = df.dominant_topic.apply(lambda x: get_topic_name(x, topic_dict))\n",
    "    print('Complete. Number of documents with topic allocated: {}'.format(len(df)))    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary topics in same sequential order \n",
    "# based on clusters and word distributions in STTM model above\n",
    "\n",
    "topic_dict = {}\n",
    "topic_names = ['health insurance',\n",
    "               'virus/outbreaks',\n",
    "               'cancer studies affecting woman/babies',\n",
    "               'miscellaneous studies affecting women/children',\n",
    "               'cancer and heart disease',\n",
    "               'diet and excercise',\n",
    "               'health and medical workers',\n",
    "               'abortion',\n",
    "               'vaping and cigarettes',\n",
    "               'drug costs and opioid crisis']\n",
    "\n",
    "for i, topic_num in enumerate(topic_indices):\n",
    "    topic_dict[topic_num]=topic_names[i]\n",
    "    \n",
    "# Allocate topics to original dataframe \n",
    "topic_allocation(tweets_df, docs, mgp, topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['num_clusters'] = 10\n",
    "tweets_df[['num_clusters','text', 'dominant_topic','topic_name']].sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat Map Visualizations by Year/User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-index dataframe by user(news source)\n",
    "reindexed_tweets = tweets_df['dominant_topic'] \n",
    "reindexed_tweets.index = tweets_df['username'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Helper function\n",
    "def keys_to_counts(keys):\n",
    "    '''\n",
    "    returns a tuple of topic categories and their \n",
    "    accompanying magnitudes for a given list of keys\n",
    "    '''\n",
    "    count_pairs = sorted(Counter(keys).items(), reverse=False)\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate counts of tweets per topic across each news source\n",
    "n_topics = mgp.K\n",
    "news_sources = tweets_df['username'].unique() \n",
    "\n",
    "user_keys = []\n",
    "for source in news_sources:\n",
    "    user_keys.append(reindexed_tweets[source].values)\n",
    "    \n",
    "user_counts = []\n",
    "for keys in user_keys:\n",
    "    categories, counts = keys_to_counts(keys)\n",
    "    user_counts.append(counts)\n",
    "\n",
    "user_topic_counts = pd.DataFrame(np.array(user_counts), index=news_sources)\n",
    "user_topic_counts.columns = ['Topic {}'.format(i) for i in range(n_topics)]\n",
    "user_topic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column to sum total topics \n",
    "user_topic_counts['total_topics'] =  user_topic_counts.sum(axis=1)\n",
    "\n",
    "# Convert topic counts to percentages for each news source \n",
    "user_topic_counts_ratio =  user_topic_counts.apply(lambda x: (x / user_topic_counts['total_topics']))\n",
    "user_topic_counts_ratio = user_topic_counts_ratio.drop(columns=['total_topics'])\n",
    "user_topic_counts_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display heat map of topics vs news sources\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(user_topic_counts_ratio, cmap=\"YlGnBu\", ax=ax);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime\n",
    "tweets_df['date'] = pd.to_datetime(tweets_df['date'], errors='coerce')\n",
    "\n",
    "# Add column for year\n",
    "tweets_df['year'] = tweets_df['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-index dataframe by year\n",
    "reindexed_tweets = tweets_df['dominant_topic']\n",
    "reindexed_tweets.index = tweets_df['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate counts of tweets per topic across years\n",
    "n_topics = mgp.K\n",
    "years_range = sorted(tweets_df['year'].unique(), reverse=False)\n",
    "\n",
    "yearly_keys = []\n",
    "for year in years_range:\n",
    "    yearly_keys.append(reindexed_tweets[year].values)\n",
    "    \n",
    "yearly_counts = []\n",
    "for keys in yearly_keys:\n",
    "    categories, counts = keys_to_counts(keys)\n",
    "    yearly_counts.append(counts)\n",
    "\n",
    "yearly_topic_counts = pd.DataFrame(np.array(yearly_counts), index=range(2014,2020+1))\n",
    "yearly_topic_counts.columns = ['Topic {}'.format(i) for i in range(n_topics)]\n",
    "\n",
    "yearly_topic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display heat map of topics vs years\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "sns.heatmap(yearly_topic_counts, cmap=\"YlGnBu\", ax=ax);\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
